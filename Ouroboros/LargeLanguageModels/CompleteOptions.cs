using System.Collections.Generic;

namespace Ouroboros.LargeLanguageModels;
public class CompleteOptions
{
    /// <summary>
    ///     The suffix that comes after a completion of inserted text.
    /// </summary>
    public string? Suffix { get; set; }

    /// <summary>
    ///     The maximum number of <a href="https://beta.openai.com/tokenizer">tokens</a> to generate in the completion.
    ///     The token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context
    ///     length of 2048 tokens (except davinci-codex and davinci-003, which support 4096).
    /// </summary>
    /// <see cref="!:https://beta.openai.com/docs/api-reference/completions/create#completions/create-max_tokens" />
    public int? MaxTokens { get; set; }

    /// <summary>
    ///     An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the
    ///     tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are
    ///     considered.
    ///     We generally recommend altering this or temperature but not both.
    /// </summary>
    public float? TopP { get; set; }

    /// <summary>
    ///     Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop
    ///     sequence.
    /// </summary>
    public string? Stop { get; set; }

    /// <summary>
    ///     Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop
    ///     sequence.
    /// </summary>
    public IList<string>? StopAsList { get; set; }
        
    /// <summary>
    ///     Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far,
    ///     increasing the model's likelihood to talk about new topics.
    /// </summary>
    /// <seealso cref="!:https://beta.openai.com/docs/api-reference/parameter-details" />
    public float? PresencePenalty { get; set; }

    /// <summary>
    ///     Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so
    ///     far, decreasing the model's likelihood to repeat the same line verbatim.
    /// </summary>
    /// <seealso cref="!:https://beta.openai.com/docs/api-reference/parameter-details" />
    public float? FrequencyPenalty { get; set; }

    /// <summary>
    ///     Generates best_of completions server-side and returns the "best" (the one with the lowest log probability per
    ///     token). Results cannot be streamed.
    ///     When used with n, best_of controls the number of candidate completions and n specifies how many to return – best_of
    ///     must be greater than n.
    ///     Note: Because this parameter generates many completions, it can quickly consume your token quota.Use carefully and
    ///     ensure that you have reasonable settings for max_tokens and stop.
    /// </summary>
    public int? BestOf { get; set; }

    /// <summary>
    ///     Modify the likelihood of specified tokens appearing in the completion.
    ///     Accepts a json object that maps tokens(specified by their token ID in the GPT tokenizer) to an associated bias
    ///     value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to
    ///     token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact
    ///     effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values
    ///     like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    ///     As an example, you can pass { "50256": -100}
    ///     to prevent the endoftext token from being generated.
    /// </summary>
    /// <seealso cref="!:https://beta.openai.com/tokenizer?view=bpe" />
    public object? LogitBias { get; set; }

    public OuroModels Model { get; set; } 

    /// <summary>
    ///     What
    ///     <a href="https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277">sampling temperature</a>
    ///     to use. Higher values means the model will take more risks. Try 0.9 for more creative
    ///     applications, and 0 (argmax sampling) for ones with a well-defined answer.
    ///     We generally recommend altering this or top_p but not both.
    /// </summary>
    /// <see cref="!:https://beta.openai.com/docs/api-reference/completions/create#completions/create-temperature" />
    public float? Temperature { get; set; }

    /// <summary>
    ///     A unique identifier representing your end-user, which will help OpenAI to monitor and detect abuse.
    /// </summary>
    public string? User { get; set; }

    public CompleteOptions()
    {
        // Defaults
        MaxTokens = 256;
        Temperature = 0.7f;
        TopP = 1;
        BestOf = 1;
        Model = OuroModels.TextDavinciV3;
    }
}